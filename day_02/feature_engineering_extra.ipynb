{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "antique-drunk",
   "metadata": {},
   "source": [
    "## Feature engineering: fitting non-linear data\n",
    "### Extra practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-supplier",
   "metadata": {},
   "source": [
    "In this notebook, we will see, how to handle cases, when it's impossible to linearly separate data.\n",
    "\n",
    "At first, let's create linearly inseparable dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "moons_points, moons_labels = make_moons(n_samples=500, noise=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-rubber",
   "metadata": {},
   "source": [
    "Let's visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(moons_points[:, 0], moons_points[:, 1], c=moons_labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-electronics",
   "metadata": {},
   "source": [
    "First of all, let's try to fit simple Logistic Regression model and see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-mileage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you got ModuleNotFoundError:\n",
    "# !pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-testimony",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "from mlxtend.plotting import plot_decision_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_lr = {\n",
    "    'penalty' : ['l1', 'l2'],\n",
    "    'C' : np.linspace(0.1, 10, num=50)\n",
    "}\n",
    "grid_lr = GridSearchCV(\n",
    "    LogisticRegression(solver='liblinear'), param_grid_lr, cv=3, verbose=1)\n",
    "grid_lr.fit(moons_points, moons_labels);\n",
    "lr = grid_lr.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-dependence",
   "metadata": {},
   "source": [
    "Function to visualize decision boundaries. If model not fitted yet, It will fit model and then plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decisions(X, y, estimator, scorer, score_name, title, figsize=(16,9), show_figure=True):\n",
    "    try:\n",
    "        estimator.predict(X)\n",
    "    except NotFittedError as e:\n",
    "        estimator.fit(X, y)\n",
    "    \n",
    "    score = scorer(y, estimator.predict(X))\n",
    "    \n",
    "    if show_figure: plt.figure(figsize=figsize)\n",
    "    plt.title(f\"{title} ({score_name} - {score})\")\n",
    "    plot_decision_regions(X, y, estimator, legend=0)\n",
    "    if show_figure: plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decisions(\n",
    "    moons_points, moons_labels,\n",
    "    lr, accuracy_score, \"Accuracy\", \n",
    "    \"Logistic Regression\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-leisure",
   "metadata": {},
   "source": [
    "What is we could use Logisitc Regression to successfully solve this task?\n",
    "\n",
    "Feature generation is a thing to help here. Different techniques of feature generation are used in real life, for now we will present one of the simpliest ways\n",
    "\n",
    "In particular case simple [PolynomialFeatures](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) are able to save the day.\n",
    "\n",
    "Generate the set of new features, train LR on it, plot decision regions, calculate metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-anderson",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "degrees = np.arange(1, 7)\n",
    "plt.figure(figsize=(12, 18))\n",
    "\n",
    "for i, degree in tqdm(enumerate(degrees)):\n",
    "    plt.subplot(3,2,i+1)\n",
    "    plot_decisions(moons_points, moons_labels, \n",
    "                   Pipeline([\n",
    "                       ('polynomial_features', PolynomialFeatures(degree=degree)),\n",
    "                       ('lr_clf', lr)\n",
    "                   ]),\n",
    "                   accuracy_score, f\"Accuracy with degree {degree}\",\n",
    "                   f\"degree = {degree}\", show_figure=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-lease",
   "metadata": {},
   "source": [
    "#### Harder Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-howard",
   "metadata": {},
   "source": [
    "Let's make this task a bit more challenging via upgrading dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-james",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "circles_points, circles_labels = make_circles(n_samples=500, noise=0.06, random_state=42)\n",
    "points = np.vstack((circles_points*2.5 + 0.5, moons_points))\n",
    "labels = np.hstack((circles_labels, moons_labels + 2))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(points[:, 0], points[:, 1], c=labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-hungary",
   "metadata": {},
   "source": [
    "To start it is worthwhile to split the sample in order to\n",
    "adequately evaluate the metrics and model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    points, \n",
    "    labels,\n",
    "    test_size=0.25,\n",
    "    random_state=42, \n",
    "    stratify=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-teddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for handy printing\n",
    "def show_accuracy(estimator):\n",
    "    pred_test = estimator.predict(x_test)\n",
    "    pred_train = estimator.predict(x_train)\n",
    "    print(f\"Accuracy on test data : {accuracy_score(y_test, pred_test)}\")\n",
    "    print(f\"Accuracy on train data : {accuracy_score(y_train, pred_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-still",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin with logistic regression. Obviously, we won't get anything good, but just look at it\n",
    "param_grid_lr = {\n",
    "    'penalty' : ['l1', 'l2'],\n",
    "    'C' : np.linspace(0.1, 10, num=50)\n",
    "}\n",
    "grid_lr = GridSearchCV(\n",
    "    LogisticRegression(solver='liblinear', multi_class='ovr'), \n",
    "    param_grid_lr,\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_lr.fit(x_train, y_train);\n",
    "best_lr_clf = grid_lr.best_estimator_\n",
    "\n",
    "show_accuracy(best_lr_clf)\n",
    "plot_decisions(\n",
    "    points, \n",
    "    labels, \n",
    "    best_lr_clf, \n",
    "    accuracy_score, \n",
    "    \"Accuracy\", \n",
    "    \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-rebecca",
   "metadata": {},
   "source": [
    "Okay, let's see, what we can do with Polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr_with_pf = Pipeline([\n",
    "    (\"pol_features\", PolynomialFeatures()),\n",
    "    (\"lr_clf\", LogisticRegression(solver=\"liblinear\", multi_class=\"ovr\"))\n",
    "])\n",
    "param_grid_lr_with_pf = {\n",
    "    'pol_features__degree' : np.arange(5, 7),\n",
    "    'lr_clf__penalty' : ['l1', 'l2'],\n",
    "    'lr_clf__C' : np.linspace(0.1, 10, num=10)\n",
    "}\n",
    "\n",
    "grid_lr_with_pf = GridSearchCV(\n",
    "    pipeline_lr_with_pf, \n",
    "    param_grid_lr_with_pf,\n",
    "    cv=3, verbose=1, n_jobs=-1)\n",
    "grid_lr_with_pf.fit(x_train, y_train);\n",
    "best_lr_with_pf = grid_lr_with_pf.best_estimator_\n",
    "\n",
    "show_accuracy(best_lr_with_pf)\n",
    "plot_decisions(\n",
    "    points, labels,\n",
    "    best_lr_with_pf, \n",
    "    accuracy_score, \"Accuracy\", \n",
    "    \"Logistic Regression with Scaler and Polynomial Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-packet",
   "metadata": {},
   "source": [
    "Okay, now we know how to create new features for sklearn pipeline. The problem is - with PolynomialFeatures transformer we can only create polynomial features ($x_1^2, x_2^2, 2x_1x_2, \\dots$) - what if we want to make different functions (for example $sin(x)$?). Well, in that case [FunctionTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) can help us. Let's see, how it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-upper",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will add 3 extra features for our data\n",
    "def add_extra_features(X):\n",
    "    return np.hstack((X, np.sin(X), np.cos(X), np.exp(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr_with_pf_and_fe = Pipeline([\n",
    "    ('add_features', FunctionTransformer(add_extra_features, validate=True)),\n",
    "    ('pol_features', PolynomialFeatures()),\n",
    "    ('lr_clf', LogisticRegression(solver='liblinear', multi_class='ovr'))\n",
    "])\n",
    "param_grid_lr_with_pf_and_fe = {\n",
    "    'pol_features__degree' : np.arange(3, 4),\n",
    "    'lr_clf__C' : np.linspace(0.1, 10, num=10)\n",
    "}\n",
    "grid_lr_with_pf_and_fe = GridSearchCV(\n",
    "    pipeline_lr_with_pf_and_fe, \n",
    "    param_grid_lr_with_pf_and_fe,\n",
    "    cv=3, verbose=10, n_jobs=-1)\n",
    "grid_lr_with_pf_and_fe.fit(x_train, y_train);\n",
    "best_lr_with_pf_and_fe = grid_lr_with_pf_and_fe.best_estimator_\n",
    "\n",
    "show_accuracy(best_lr_with_pf_and_fe)\n",
    "plot_decisions(\n",
    "    points, labels, \n",
    "    best_lr_with_pf_and_fe, accuracy_score, \n",
    "    \"Accuracy\", \n",
    "    \"Logistic regression with Scaler, Polynomial Features and Feature engeneering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-helping",
   "metadata": {},
   "source": [
    "At last, It often benefitial to normalize our data. Let's try add scaler to our pipeline and see final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr_with_pf_and_fe = Pipeline([\n",
    "    ('add_features', FunctionTransformer(add_extra_features, validate=True)),\n",
    "    ('pol_features', PolynomialFeatures()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr_clf', LogisticRegression(solver='liblinear', multi_class='ovr'))\n",
    "])\n",
    "param_grid_lr_with_pf_and_fe = {\n",
    "    'pol_features__degree' : np.arange(3, 4),\n",
    "    'lr_clf__C' : np.linspace(0.1, 10, num=10),\n",
    "    'scaler': [StandardScaler(), MinMaxScaler()]\n",
    "}\n",
    "grid_lr_with_pf_and_fe = GridSearchCV(\n",
    "    pipeline_lr_with_pf_and_fe, \n",
    "    param_grid_lr_with_pf_and_fe,\n",
    "    cv=3, verbose=1, n_jobs=-1)\n",
    "grid_lr_with_pf_and_fe.fit(x_train, y_train);\n",
    "\n",
    "best_lr_with_pf_and_fe = grid_lr_with_pf_and_fe.best_estimator_\n",
    "show_accuracy(best_lr_with_pf_and_fe)\n",
    "\n",
    "plot_decisions(\n",
    "    points, labels, best_lr_with_pf_and_fe, accuracy_score, \"Accuracy\", \n",
    "    \"Logistic regression with Scaler, Polynomial Features and Feature engeneering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-pharmacology",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-tattoo",
   "metadata": {},
   "source": [
    "### TBD: Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-local",
   "metadata": {},
   "source": [
    "Actually, when creating extra features with FunctionTransformer and PolynomialFeatures we create a lot of new features. Let's see that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_pipe = Pipeline([\n",
    "    ('add_features', FunctionTransformer(add_extra_features, validate=True)),\n",
    "    ('pol_features', PolynomialFeatures(2)),\n",
    "])\n",
    "\n",
    "transf = fe_pipe.fit_transform(x_train)\n",
    "print(f\"Num of features before fe: {x_train.shape[1]}\")\n",
    "print(f\"Num of features after fe (degree=2): {transf.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-milwaukee",
   "metadata": {},
   "source": [
    "It is just using `degree=2`. See, what happens, when we increase degree..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_pipe = Pipeline([\n",
    "    ('add_features', FunctionTransformer(add_extra_features, validate=True)),\n",
    "    ('pol_features', PolynomialFeatures(6)),\n",
    "])\n",
    "\n",
    "transf = fe_pipe.fit_transform(x_train)\n",
    "print(f\"Num of features before fe: {x_train.shape[1]}\")\n",
    "print(f\"Num of features after fe (degree=2): {transf.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-miniature",
   "metadata": {},
   "source": [
    "Intuitevly, we don't need so many features. Let's see correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_pipe = Pipeline([\n",
    "    ('add_features', FunctionTransformer(add_extra_features, validate=True)),\n",
    "    ('pol_features', PolynomialFeatures(2)),\n",
    "])\n",
    "\n",
    "transf = fe_pipe.fit_transform(x_train)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    np.hstack([transf, y_train[:, None]]), \n",
    "    columns=[str(i) for i in range(transf.shape[1])] + [\"target\"])\n",
    "\n",
    "sns.heatmap(\n",
    "    df.corr(),cmap='RdYlGn',linewidths=0.1\n",
    ")\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(15,15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-chart",
   "metadata": {},
   "source": [
    "Okay, there are features, which are useless. We can easily discard them and we won't have any decline in metrics (sometimes even otherwise we'll see score increasing).\n",
    "\n",
    "Technique of choosing, with features are relevant and which aren't is called [Feature Selection](https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/). We will see, how it works in future (for now, you can read blogpost about it). \n",
    "\n",
    "Stay tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-railway",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
